---
title: CSC3100 Week8-10
date: 2019-10-13 10:04:31
tags: 
  - CSC3100
  - Data Structure
  - Algorithm
  - Trees
categories:
  - note
  - CSC3100
mathjax: true
---

Trees

## Definitions

- A tree is a collection of nodes.

- The collection can be empty .

- Otherwise, a tree consists of a distinguished node *r*, called root, and zero or more nonempty (sub) trees $T_1$, $T_2$, …$T_k$, each of whose roots are connected by a direct edge from *r*.

- A tree with *N* nodes has one root, and *N-1* edges 

### Root

The top most node of the tree.

### Edge

Nodes other than root in a tree.

### Parent

Node A is the parent of node B if B is the root of the left or right sub-tree of A.

### Child

Node B is the left (right) child of node A if A is the parent of B.

### Sibling

Node B and node C are siblings if they have the same parent.

### Leaf

A node is called a leaf if it has no children

### Path

A path from node $n_1$ to $n_k$: A sequence of nodes $n_1$, $n_2$, …, $n_k$ such that ni is the parent of $n_{i+1}$ for $1 \leq i < k$.

#### Length of a path

- The length of this path is the number of edges on the path, namely k-1.

- Notice that in a tree, there is exactly one path from the root to each node.

### Depth

Depth of a node $n_i$ is the length of the unique path from the root to $n_i$. 

The root is at depth 0.

### Height

Height of $n_i$ is the length of the longest path from $n_i$ to a leaf.

All leaves are at height 0.

## Binary Tree

**A Binary Tree is a tree** in which no node can have more than two children (subtrees): $T_L$ and $T_R$, both of which could possibly be empty.

**Strictly Binary Tree:** Every non-leaf node in the tree has nonempty left and right sub-trees

**Complete (full) binary tree:** A complete binary tree of depth *d* is a strictly binary tree with all leaf nodes at level *d*.

### Expression Tree

$(a+b\times c) +((d\times e+f)\times g)$

![1](/1.png)

1. The leaves of an expression tree are operands;

2. The other nodes contain operators.

### Traversing Strategy

#### Preorder (depth-first)

- Visit the node

- Traverse the left subtree in preorder

- Traverse the right subtree in preorder

#### Inorder

- Traverse the left subtree in inorder

- Visit the node

- Traverse the right subtree in inorder

#### Postorder

- Traverse the left subtree in postorder

- Traverse the right subtree in postorder

- Visit the node

## Binary Search Tree

**Tree representation:**

- A linked data structure in which each node is an object

**Node representation:**

- Key field

- Satellite data

- Left: pointer to left child

- Right: pointer to right child

- p: pointer to parent (p[root[T]] = NIL)

### Property

For every node, T, in the tree:

- the key values in its left subtree are **all** *smaller than* the key value of T. 
- the key values in its right subtree are **all** *larger than* the key value of T.

### Operations

Binary Tree support many dynamic set operations:

find, findMin, findMax, predecessor, successor, insert, delete

### Running Time

**On Average:** $\Theta (\log n)$ The expected height of the tree is logn

**Worst Case:** $\Theta (n)$ The tree is a linear chain of *n* nodes.

### Search

Starting at the root, trace down a path by comparing k with the key of the current node:

- If the keys are equal: we have found the key
- If k < key[x] search in the left subtree of x
- If k > key[x] search in the right subtree of x

**Running Time:** O(h)

### Finding the Minimum

Following left child pointers from the root, until a NIL is encountered

### Finding the Maximum

Following right child pointers from the root, until a NIL is encountered  

### Successor

#### Definition

successor(x) = y, such that key[y] is the smallest key *>* key[x]

#### Case 1

**right(x) is not empty:**

successor(x) = the minimum in right(x)

#### Case 2

**right(x) is empty:**

- go up the tree until the current node is a left child: successor(x) is the parent of the current node

- if you cannot go further (and you reached the root):  x is the largest element

### Predecessor

#### Definition

predecessor(x) = y, such that key[y] is the biggest key < key[x]

#### Case 1

**left(x) is not empty:**

predecessor(x) = the maximum in left(x)

#### Case 2

**left(x) is empty:**

- go up the tree until the current node is a right child: predecessor(x) is the parent of the current node

- if you cannot go further (and you reached the root):  x is the smallest element

### Insert

#### Goal

Insert value v into a binary search tree

#### Method

- If key[x] < v move to the right child of x, else move to the left child of x

- When x is NIL, we found the correct position 

- If v < key[y] insert the new node as y’s left child else insert it as y’s right child

- Beginning at the root, go down the tree and maintain:
  - Pointer x : traces the downward path (current node)
  - Pointer y : parent of x (“trailing pointer”)

### Deletion

#### Goal

Delete a given node z from a binary search tree

#### Idea

##### Case 1

**z has no children**

Delete z by making the parent of z point to NIL

##### Case 2

**z has one child**

Delete z by making the parent of z point to z’s child, instead of to z and link the parent with the new child

##### Case 3

**z has two children**

- Find z’s *successor y* (the leftmost node in z’s right subtree)

- y has either no children or one right child (but no left child), why?

- Delete y from the tree (via Case 1 or 2)

- Replace z’s key and satellite data with y’s

### Summary

#### Operations on Binary Search Trees

| Operations  | Time Complexity |
| :---------: | --------------- |
|   SEARCH    | O(h)            |
| PREDECESSOR | O(h)            |
|  SUCCESOR   | O(h)            |
|   MINIMUM   | O(h)            |
|   MAXIMUM   | O(h)            |
|   INSERT    | O(h)            |
|   DELETE    | O(h)            |

**Note:** These operations are fast if the height of the tree is small – otherwise their performance is similar to that of a linked list.

### Issues

After a series of DELETION, the above algorithm favors making the *left sub-trees* **deeper** than the *right sub-trees*.

#### Possible Solution

Try to eliminate the problem by randomly choosing between the smallest element in the right sub-tree and the largest in the left when replacing the deleted element (not rigorous and not prove it yet!!)

#### Proven Solution

- Red-Black Tree
- AVL Tree

## Red-Black Tree

- Binary search tree with an additional attribute for its nodes: color which can be **<span style="color: red">red</span>** or **black**

- Restrict the way nodes can be colored on any path from the root to a leaf.

- Ensures that no path is more than twice as long as any other path.

- “Balanced” binary search trees guarantee an O(logn) running time.

### Properties

1. Every node is either red or black
2. The root is black
3. Every leaf (NIL) is black
4. If a node is red, then both its children are black. No two consecutive red nodes on a simple path from the root to a leaf
5. For each node, all paths from that node to descendant leaves contain the same number of black nodes

#### Black Height

**Height** of a node: the number of edges in the **longest** path to a leaf

**Black-height** of a node x: bh(x) is the number of black nodes (including NIL) on the path from x to a leaf, ***not counting x***

![image-20191110105701357](/image-20191110105701357.png)

### Height of Red-Black Tree

#### Important Property of Red-Black Tree

A red-black tree with n internal nodes has height **at most** 2log(n+1)

Need to prove two claims first:

##### Claim 1

Any node x with height h(x) has $bh(x) \geq \frac{h(x)}{2}  $

**Proof:**

By Properties 4, at most $\frac{h}{2}$ **<span style="color: red">red</span>** nodes on the path from the node to a leaf. So at least $\frac{h}{2}$ are **black**.

##### Claim 2

The subtree rooted at any node x contains **at least** $2^{bh(x)} - 1$ internal nodes. (Internal Nodes = All Nodes - Leaves)

**Ptoof:** By induction on h[x]

**Basis:** 

When h[x] = 0, x is a leaf, so bh(x) = 0 and the number of internal nodes is $0 = 2^0-1$

**Inductive Hypothesis:** Assume it is true for h[x]=h-1

**Inductive Step:** 

Let bh(x) = b, then any child y of x has: 

- bh(y) = b(if the child is **<span style="color: red">red</span>**)
- bh(y) = b - 1(if the child is **black**)

Using inductive hypothesis, the number of internal nodes for each child of x is at least (if it is **black**): $2^{bh(x) - 1} - 1$

The subtree rooted at x contains at least:
$$
\begin{align} 
& (2^{bh(x) - 1} – 1) + (2^{bh(x) - 1} – 1) + 1\\ 
= & 2 \times (2^{bh(x) - 1} - 1) + 1\\
= & 2^{bh(x)} - 1
\end{align}
$$
internal nodes

##### Proof

$$
n \geq 2^{bh}-1 \geq 2^{\frac{h}{2}}-1
$$

Using Claims.

Add 1 to both sides and then take logs:
$$
\begin{align} 
& n+1 \geq 2^{bh} \geq 2^{\frac{h}{2}}\\
\Rightarrow & \lg(n+1) \geq \frac{h}{2}\\
\Rightarrow & h \leq 2 \lg(n+1)
\end{align}
$$

### Operations of Red-Black Tree

The non-modifying operations: **MINIMUM**, **MAXIMUM**, and **SEARCH** still run in O(h) time. They take O(logn) time on red-black trees because Red-Black Tree ensure that the height of the tree will be O(logn)

For **INSERT** and **DELETE**, we have to guarantee that the modified tree will still be a Red-Black Tree.

#### Insert

What color to make the new node?

##### Red

Property 4 *may be* violated: if a node is red, then both its children are black.

##### Black

Property 5 *may be* violated: all paths from a node to its leaves contain the same number of black nodes  

#### Delete

What is the color of the node to be removed?

##### Red

All properties will be fulfilled

##### Black

Property 2 *may be* violated: The root is **black**

Property 4 *may be* violated: If a node is red, then both its children are **black**

Property 5 *may be* violated: For each node, all paths from the node to descendant leaves contain the same number of black nodes.

#### Rotations

Operations for re-structuring the tree after insert and delete operations on red-black trees.

Rotations take a red-black-tree and a node within the tree and:

- Together with some node re-coloring them and help restore the red-black-tree property

- Change some of the pointer structure

- **Do not** change the binary-search tree property 

##### Two Types of Rotations

###### Left Rotations

Assumptions for a left rotation on a node x: The right child of x (y) is not NIL.

![image-20191110165046936](/image-20191110165046936.png)

**Idea:** 

- Pivots around the link from x to y

- Makes y the new root of the subtree

- x becomes y’s left child

- y’s left child becomes x’s right child
- x' left child and y's right child stays the same

###### Right Rotations

Assumptions for a right rotation on a node x: The left child of y (x) is not NIL

![image-20191110175535986](/image-20191110175535986.png)

**Idea:**

- Pivots around the link from y to x
- Makes x the new root of the subtree
- y becomes x’s right child
- x’s right child becomes y’s left child

#### Insert with Rotation

##### Goal

Insert a new node z into a red-black-tree  

##### Idea

- Insert node z into the tree as for an ordinary binary search tree
- Color the node **<span style="color: red">red</span>**
- Restore the red-black-tree properties
  - Use an auxiliary procedure RB-INSERT-FIXUP

From the above, we know that Property 4: If a node is red, then both its children are black may be violated. So we need to fix it.

##### INSERT FIXUP

###### Case 1

z's ***uncle*** is **<span style="color: red">red</span>**

![image-20191110183705840](/image-20191110183705840.png)

![image-20191110183720248](/image-20191110183720248.png)

p[p[z]] (z’s grandparent) must be black.

**No matter z is left child or right child:**

- Color p[z] **black**
- Color y **black**
- Color p[p[z]] **<span style="color: red">red</span>**
- z = p[p[z]]
- And Repeat until Case 1 would not happen again
- Push the **<span style="color: red">"red"</span>** violation up the tree

###### Case 2

z's ***uncle*** is **black** and z is a right child

![image-20191110183753132](/image-20191110183753132.png)

**Idea:**

- z = p[z]

- LEFT-ROTATE(T, z)

now z is a left child, and both z and p[z] are red  

Then **Case 2** is turned into **Case 3**

###### Case 3

z's ***uncle*** is **black** and z is a left child

![image-20191110183741489](/image-20191110183741489.png)

**Idea:**

- color p[z] to **black**
- color p[p[z]] to **<span style="color: red">red</span>**
- RIGHT-ROTATE(T, p[p[z]])

No longer have two **<span style="color: red">red</span>**s in a row

So the operation is done.

##### Analysis of INSERT

Inserting the new element into the tree O(logn). 

For RB-INSERT-FIXUP, the while loop repeats only if **Case 1** is executed, the number of times the while loop can be executed is O(logn).

So total running time of RB-INSERT is O(logn)

### Summary

- Operations on red-black-trees:

| Operations  | Time Complexity |
| ----------- | --------------- |
| SEARCH      | O(h) / O(logn)  |
| PREDECESSOR | O(h) / O(logn)  |
| SUCCESSOR   | O(h) / O(logn)  |
| MINIMUM     | O(h) / O(logn)  |
| MAXIMUM     | O(h) / O(logn)  |
| INSERT      | O(h) / O(logn)  |
| DELETE      | O(h) / O(logn)  |

- Red-black-trees guarantee that the height of the tree will be O(logn)

## AVL Tree

- An AVL tree is a binary search tree with a balance condition
- Invented by Georgy Adelson-Velsky and Evgenii Landis
- The balance condition: **The height of the left and right subtrees can differ by at most 1.** 
- In an AVL Tree, height **or** balancing information is kept for each node
- All the tree operation can be performed in O(logn) time.


### Tree Height

- N(h) = minimum number of nodes in an AVL tree of height h 
- N(h) = N(h-1)+N(h-2)+1
- N(0) = 1, N(1) = 2
- $N(h) = \phi^h$ ($\phi \approx $1.62)
- Suppose we have n nodes in  an AVL tree of height h: $n \geq N(h) \geq h \leq 1.44\log(n)$
- The height of an AVL Tree is at most roughly <span style="color:red">$1.44\log(n)$</span>

### Node Height

**Balance Factor:** $h_{\text{left}}-h_{\text{right}}$

**empty height** (height of empty node): -1

### Insert

- When we do an insertion, we need to update all the balancing (or height) information for the nodes on the path back to the root. Inserting a node may violate the AVL tree property.

- After an insertion, only nodes that are on the path from the insertion point to the root might have their balance altered.
- As we follow the path up to the root and update the balancing information, we may find a node whose new balance violates the AVL condition.

- Rebalancing the tree at the first (i.e., deepest) such node can guarantee the entire tree satisfying the AVL property.

Let the node that needs rebalancing be <span style="color:red">$\alpha$</span>. There are 4 cases:

**Outside Cases:** (require single rotation)

1. Insertion into **left** subtree of **left** child of $\alpha$ (LL).

2. Insertion into **right** subtree of **right** child of $\alpha$ (RR).

**Inside Cases:** (require double rotation)

3. Insertion into **right** subtree of **left** child of $\alpha$ (RL).

4. Insertion into **left** subtree of **right** child of $\alpha$ (LR).

The rebalancing is performed through four seperate rotation algorithms.

**Note:** the height of the tree is h+2 before the insertion

![image-20191110194207045](/image-20191110194207045.png)

#### Case 1

![image-20191110194258229](/image-20191110194258229.png)

Do RIght-Rotation and then done! AVL properties has been restored.

**Note:** the height of this tree is still h+2 after the fixup.

#### Case 4

![image-20191110194609510](/image-20191110194609510.png)

![image-20191110195115921](/image-20191110195115921.png)

![image-20191110195131342](/image-20191110195131342.png)

![image-20191110195140139](/image-20191110195140139.png)

**Note:** the height of this tree is still h+2 after the fixup.

#### Case 2 and Case 3

For these two cases, it's just mirror operation of **Case 1** and **Case 4**

### Red-Black Tree vs. AVL Tree

- AVL trees maintain a more tight balance than red-black trees. 
  - Root height: ~1.44 log(n) vs ~2 log (n)

- lookup in an AVL tree is typically faster, but this comes at the cost of slower insertion and deletion due to more rotation operations.

- AVL tree requires **O(N) extra space** for height whereas Red Black tree requires only 1 bit for color

- So using an AVL tree if you expect the number of lookups to dominate the number of updates to the tree.

- Red Black trees are used in most of the language libraries like map, multimap, multiset in C++ whereas AVL trees are used in databases where faster retrievals are required.

## Large Data Application

- insert: AVL tree is faster because you need to lookup for a particular node before insertion. AVL tree & RB tree still only need constant number of rotation at the worst case. Thus the bottle neck will become the time you lookup for that particular node.
- lookup: AVL tree is faster. 
- delete: AVL tree is faster on average, similar to the reason of insertion). 

## B-Tree

### Motivation for B-Trees

#### Data Layout on Disk

- Track: one ring

- Sector: one pie-shaped piece.

- Block: intersection of a track and a sector – minimum unit for disk storage.

![image-20191110155018957](/image-20191110155018957.png)

#### Disk Block Access Time

- Seek Time = Time for the disk head to move to the correct track + Time for the beginning of the correct sector to spin round to the head. 
- Transfer Time = Time to read or write the data in blocks.  
- For a 7200 RPM hard disk, one revolution occurs in 1/120 of a second. Seek time is about 8.4 ms and average transfer time for a block is about 12 ms. 
- Random Access Memory (RAM) takes nanoseconds to read from or write to, 100,000 times faster than disk.

#### Motivation

- Index structures for large datasets cannot be stored in main memory

- Storing it on disk requires different approach to efficiency. (Can’t afford too many disk access)

- The disk access needs to be optimized. (One access reads more data)

- Need an efficient disk based indexing method.

#### Limitation of Binary Tree

- Assume that we use an AVL tree to store about 20 million records and all indexes are stored on disk.

- A **very** deep binary tree with lots of different disk accesses; log2 20,000,000 is about 24, so this takes about 0.3 seconds 

- The log*n* lower bound on search for a binary tree. How to improve?

- The solution is to use more branches and thus reduce the height of the tree!
  - As branching increases, depth decreases

### B-Trees

e.g. B-tree of order 4:
![image-20191110155553886](/image-20191110155553886.png)

**Note:** B-Tree do not require that each node always be full. Empty space will permit insertion without rebalancing. Allowing empty space after a deletion can also avoid rebalancing.

- A B-tree of order m is an m-way tree (i.e., a tree where each node may have up to m children) in which:

1. The number of keys in each non-leaf node is one less than the number of its children and these keys partition the keys in the children in the fashion of a search tree

2. All leaves are on the same level

3. All non-leaf nodes except the root have at least $\lceil\frac{m}{2}\rceil$ children

4. The root is either a leaf node, or it has from two to m children

5. A leaf node contains no more than m–1 keys

**Note:** all nodes except root contain $[\lceil\frac{m}{2}\rceil - 1, m -1]$ elements.

### Tree-Based Indexing

- Only the last level of index tree provides the address of data in disk

- Deletion in disk is easy

- Insertion may have trouble

  - Solution: **Overflow Buckets** 

### Constructing a B-Tree

- Add each number sequentially to a correct leaf node
- If adding a new number will violate Property 5, add it in and promote the middle one to the higher level. If the higher level also violate Property 5 after that, repeat the procedure until everything is done.

### Insert

- Attempt to insert the new key into a leaf
- If that leaf becoming too big, split the leaf into two, promoting the middle key to the leaf’s parent
- If the parent becomes too big, split the parent into two, promoting the middle key
- This strategy might have to be repeated all the way to the top

### Delete

Two possible situations in deletion:

1. If the key is already in a leaf node, and removing it doesn’t cause that leaf node to have too few keys, then simply remove it.

2. If the key is *not* in a leaf then it is guaranteed (by the nature of a B-tree) that its predecessor or successor will be in a child -- delete the key and promote the predecessor or successor key to the deleted key’s original position.

**If (1) or (2) lead to a leaf node containing too few keys, look at the immediately adjacent siblings:** 

3. If one of them has more than the min. number of keys then we can promote one of its (smallest or biggest) keys to the parent and take the parent key into our lacking leaf.

4. If neither of them has more than the min. number of keys, then the lacking leaf and one of its neighbours can be combined with their shared parent; if this step leave the parent with too few keys then we repeat the process up to the root itself, if required.

#### Case 1

**Simple leaf deletion**  

![image-20191112134828693](/image-20191112134828693.png)

#### Case 2

**Simple non-leaf deletion**  

![image-20191112134854837](/image-20191112134854837.png)

#### Case 3

**Enough siblings**

![image-20191112134929183](/image-20191112134929183.png)

#### Case 4

**Too few keys in node and its siblings**  

![image-20191112134952566](/image-20191112134952566.png)

![image-20191112135000539](/image-20191112135000539.png)

### Analysis

The maximum number of items in a B-tree of order *m* and height *h*:

| Level   | Number of Items |
| ------- | --------------- |
| root    | $m – 1$         |
| level 1 | $m(m – 1)$      |
| level 2 | $m^2(m – 1)$    |
| . . .   | . . .           |
| level h | $m^h(m – 1)$    |

So, the total number of items is $(1 + m + m^2 + m^3 + … + m^h)(m-1) = (\frac{m^{h+1}-1}{m-1})(m-1) = m^{h+1}-1$

### Reasons for Using B-Trees

- When searching tables held on disc, the cost of each disc transfer is high but doesn't depend much on the amount of data transferred, especially if consecutive items are transferred

- If we use a B-tree of order 101, say, we can transfer each node in one disc read operation

- A B-tree of order 101 and height 3 can hold $101^4 – 1$ items (approximately 100 million) and any item can be accessed with 3 disc reads (assuming we hold the root in memory)

## Comparing Trees

- Binary trees
  - Can become *unbalanced* and *lose* their good time complexity (big O)
  - RB trees and AVL trees are strict binary trees that *overcome the balance problem*
  - Heaps remain balanced but only *prioritise* (not order) the keys
- Multi-way trees
  - B-Trees can be *m*-way, they can have any number of children, useful for database