---
title: CSC3100 Week6
tags:
  - CSC3100
  - Data Structure
  - Algorithm
categories:
  - note
  - CSC3100
date: 2019-09-26 16:00:44
mathjax: true
---

## Bubble Sort

1. Repeatedly pass through the array
2. Swaps adjacent elements that are out of order

**Advantage:** easier to implement

### Bubble Sort vs. Insertion Sort

In average case, Insertion Sort is about $\frac{n^2}{4}$ and Bubble Sort is about $\frac{n^2}{2}$. So Bubble Sort is not very useful in practices.

### Inversion in Analize Insertion Sort

**Inversion:** An inversion in an array of numbers is any ordered pair (i, j) having the property that i < j but A \[i\] > A \[j\].

**Theorem:** On average, there are $\frac{N(N-1)}{4}$ inversions in an array of N distinct elements.

**Proof:**

- Let L be a list for any given N distinct elements.
- Let Lr be the reverse of L
- Let (x,y) be any pair among N distinct elements.
- Then either L or Lr contains an inversion of (x,y).
- The total number of inversions of L and Lr is $\frac{N(N-1)}{2}$
- The average of L is $\frac{N(N-1)}{4}

**Conclusion:**

- Each swap between 2 adjacent elements removes exactly one inversion.
- On average, an algorithm that sorts by exchanging adjacent elements requires about N(N-1)/4 time complexity. 
- A sorting algorithm that is subquadratic must do comparisons and exchanges between elements that are far apart.

## Shell Sort

- Break the quadratic time barrier by comparing elements that are *distant*.
- The distance between comparisons decreases as the algorithm runs until the last phase, in which adjacent elements are compared (diminishing increment sort).
- An increment sequence $h_1$, $h_2$, $h_3$, ..., $h_t$, used in *reverse* order with $h_1=1$.

Many rounds of Insertion Sort

Such jumping may imporve Bubble Sort's performance, too.

Such technique can be used to combine other sorts.

### Analysis of Shell Sort

**Theorem:** The worst-case running time of Shellsort, using some increment, is $\Theta(N^2)$.

### Good Sequence for Shell Sort

**Hibbard’s increment:** 1, 3, 7, .., $2^{k-1}$

- Worst-case running time: $\Theta (N^{\frac{3}{2}})$
- Average-case running time: $O (N^{\frac{5}{4}})$

**Sedgewick’s increment:** 1, 5, 19, 41, 109, ... (each term is either $9\times 4^i-9\times 2^i+1$ or $4^i-3\times 2^i+1$)

- Worst-case running time: $O (N^{\frac{4}{3}})$
- Average-case running time: $O (N^{\frac{7}{6}})$

## Heap Sort

- First build a binary (max) heap of N elements. Which means the maximum number is at root
- Then perform N-1 DeleteMax operations by swapping the last element in the heap with the first, decrementing the heap size, and percolating down.
- Sorting in O(NlogN) time, but in practice, is slower than Shellsort that uses Sedgewick’s increment sequence.

## Merge Sort

Merge Sort is very good for sorting huge data.

Although the disk can be very big, the memory of a computer is limited(Means data you can manipulate in a time is limited).

So it's good for External Sort.

## Quick Sort

Comparison based sorting method

- Very fast known sorting algorithm in practice
- Average running time is $O(NlogN)$
- Worst case performance is $O(N^2)$ (but very unlikely)

### Algorithm

- If the number of elements in S is 0 or 1, then return.
- Pick any element v in S. This is called the pivot.
- Partition S -{v} (the remaining elements in S) into 2 disjoint groups:
  - $S1 = {x \in S-{v}|x \leq v}$
  - $S2 = {x \in S-{v}|x \geq v}$
- Return {quicksort ($S_1$) followed by v followed by quicksort ($S_2$)}.

### An Improvement

As we all know, quick sort's performance will be worse if the pivot is too big or too small, so we can take the first, final and middle element and choose the middle one to be the pivot every time.

### Limitation

- Quick Sort does not perform well for small arrays: CUTOFF is about 10. We often use Insertion Sort for such arrays.
- For every sorting algorithm that is comparison based, it must take $\Omega (nlogn)$ in worst case.

## Bucket Sort

**Assumption:** the input is generated by a random process that distributes elements uniformly over [0, 1)

**Idea:**

- Divide [0, 1) into n equal-sized buckets
- Distribute the n input values into the buckets
- Sort each bucket (e.g., using quicksort)
- Go through the buckets in order, listing elements in each one

### Running Time

Assume that elements are distributed uniformly, running time is $\Theta (n)$



## Radix Sort





### Stable Sort





## External Sort

- For the input on some sequential-access media like tapes and it is too large to be all incorporated into the memory.
- Computing time becomes insignificant when compared with access time.
- Merge routine from Merge Sort is used.
